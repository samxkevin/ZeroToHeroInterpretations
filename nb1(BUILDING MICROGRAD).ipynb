{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation For the First Video: \n",
    "[The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0?si=8FQ4c7gDBBAXCdci)\n",
    "\n",
    "This notebook has been created upon the suggestion by Neil sir to go through the Andrej Kaparthy video regarding Neural Networks.\n",
    "\n",
    "Let's Title this notebook as:\n",
    "\n",
    "# __BUILDING MICROGRAD__ \n",
    "_as it fits the video..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Index:__\n",
    "- [Takeaways](#takeaways)\n",
    "- [Micrograd Overview](#micrograd)\n",
    "    - [Explanation by an Example](#Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Takeaways__:\n",
    "By the end of this Lecture, we will understand:\n",
    "\n",
    "- The basic structure and purpose of neural networks.\n",
    "- How to define and train a neural network using Micrograd.\n",
    "- The step-by-step process of backpropagation and how it tunes weights.\n",
    "- How an autograd engine works and its role in gradient calculation.\n",
    "- Practical application and experimentation using Micrograd’s minimalistic approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Bash\n",
    "pip install numpy\n",
    "pip install micrograd\n",
    "pip install matplotlib\n",
    "```\n",
    "Execute the above commands in your shell to install the requirements for this Lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Micrograd Overview__:\n",
    "Micrograd is a small library that demonstrates how an automatic differentiation engine (autograd engine for short) works. It uses something called a computational graph (or expression graph) to keep track of operations between values. The main idea is to build this graph as you perform operations and then use [Backpropagation](#Backpropagation) to compute derivatives or gradients.\n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "a = Value(-4.0)\n",
    "b = Value(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create two Value objects: `a` with `-4.0` and `b` with `2.0`. These objects not only store their numerical values but also keep track of gradients and the operations they participate in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code `c = a + b` creates a Value object c that's the sum of `a` and `b`. This operation is stored in the computational graph, forming a simple graph with `c` dependent on `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c += c + 1\n",
    "c += 1 + c + (-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `c += c + 1`: This statement updates the value of `c` by adding `c + 1` to it. The expression graph records `c`'s dependency on its previous value, another instance of itself, and the constant `1`.\n",
    "- `c += 1 + c + (-a)`: This statement updates `c` by adding `1 + c + (-a)` to it. Now, `c` depends on the constant `1`, another instance of itself, and the negative of `a`. The computational graph continues to grow, with `c` linked to all these operations and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.data) #output: -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final value of `c` is `-1.0`, resulting from a sequence of additions and transformations involving `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backpropagation:\n",
    "When you call `c.backward()`, Micrograd computes the gradients of `c` with respect to `a` and `b` by traversing the expression graph backwards. It applies the chain rule to determine how changes in `a` and `b` influence `c`. This is known as Backpropagation. Backpropagation is an algorithm used in neural networks to compute gradients. It works by propagating the error backward through the network, applying the chain rule to calculate the gradient of the loss function with respect to each weight. This enables the network to update weights and minimize the error, improving its performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad) #output: 3.0\n",
    "print(b.grad) #output: 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result:\n",
    "\n",
    "* **Gradient of `a` (`a.grad`)**: `3.0`\n",
    "* **Gradient of `b` (`b.grad`)**: `4.0`\n",
    "\n",
    "This means:\n",
    "* A small change in `a` causes a change in `c` three times as large.\n",
    "* A small change in `b` causes a change in `c` four times as large.\n",
    "\n",
    "The expression graph tracks all dependencies, and backpropagation efficiently calculates these gradients. This process is fundamental in neural networks, allowing for weight updates during training to minimize loss. Backpropagation applies to many operations, including in Multi-Layered Perceptrons (MLPs), where inputs and weights interact through matrix operations. Starting from `c`, the gradient is computed by applying the chain rule recursively to all nodes in the expression graph influencing `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network are just mathematical expressions that take input data and weights as the input and output data are your predictions or the loss functions. But in the case of Backpropagation it is more general and it doesn't care about the network what it cares about are just mathematical expressions. And we use this for training of neural networks.\n",
    "\n",
    "So Micrograd is basically what we need to train neural networks and everything else is just efficiency(usage of n-dimensional tensors and memory allocation). So You think that it could be a very complex piece of code but that turns out to not to be the case where you see that it is just [barely 100 lines of code of simple python](https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py)\n",
    "##### __Installation__:\n",
    "Run the following command in your shell:\n",
    "`pip install micrograd`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Implementing Micrograd__:\n",
    "##### Derivative Of a Simple Function with One Input:\n",
    "Derivatives quantify how changes in an input affect the output, showing the rate of change of a function. In neural networks, partial derivatives reveal each input’s impact on the output, crucial for adjusting parameters and optimizing models. Again let's deep dive to understand this with an \n",
    "##### __Example__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def f(x):\n",
    "    return x**2-x-6\n",
    "f(3)\n",
    "abscissae=np.arange(-5,5,0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of Values of x from -5 to 5 in steps of 0.25 but not including 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abscissae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting those values we get a Parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinates=f(abscissae)\n",
    "plt.plot(abscissae,ordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Derivative of a Function__:\n",
    "The Derivative shows how a function’s value changes with a small input change.\n",
    "- If the function increases, the derivative is positive.\n",
    "- If it decreases, the derivative is negative.\n",
    "It indicates the direction and rate of change at that point.\n",
    "So,\n",
    ">$$f'(x) = \\lim_{{h \\to 0}} \\frac{{f(x + h)-f(x)}}{h}$$\n",
    "Executing this we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.001\n",
    "x=2\n",
    "(f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Deep Learning, frameworks use backpropagation to automatically compute derivatives. For example, for $ f(x)=x^2-x-6 $, the derivative is $f'(x)=2x-1$. At $x=2$, it equals $3$, which matches the code output. Automatic differentiation makes manual calculation unnecessary.\n",
    "\n",
    "Now Lets get a little bit more complex for this case i.e., $3$ inputs and $1$ output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1.0\n",
    "b=2.0\n",
    "c=3.0\n",
    "r=a*b-c\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute the Derivative of this function considering the small input change i.e., $h$ to be $0.001$ which shall be done with respect to $a$. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.0001\n",
    "a+=h\n",
    "H=a*b-c\n",
    "print((H-r)/h)\n",
    "a=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Result derived above is the Partial Derivative of the function $r$ i.e. $a*b-c$ with respect to $a$. The variable $a$ in this aspect is being restored to the original value since the derivative of the function $r$ changes as we run the above code block again and again which would be confusing to us.Which highlights a need to return variables to their initial state to avoid cumulative changes, which can otherwise misrepresent derivative calculations or confuse gradient tracking when rerunning code blocks. This happens because we are adding the value of $h$ again and again as we are rerunning the Code blocks.\n",
    ">The partial derivative shows how a function's output varies with each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Starting The Core `Value` object of Micrograd and its Visualization:__\n",
    "Now, let's move onto Neural Networks. Neural Networks consist of complex mathematical expressions managed through data structures like the `Value` object, which stores values and tracks gradients. Similar to resetting `a` after each derivative calculation, the `Value` object resets its state to avoid unintended changes, ensuring accurate gradient computations and efficient backpropagation for optimizing network parameters.\n",
    "This Chapter, we are going to build out the `Value` object. Below is the Skeleton of the `Value` Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self,other): #this is defined explicitly for the addition of two `Value` objects\n",
    "        return Value(self.data+other.data)\n",
    "    def __mul__(self,other): #this is defined explicitly for the multiplication of two `Value` objects\n",
    "        return Value(self.data*other.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__repr__` is there to make the object more understandable when printed, instead of just showing the raw memory address\n",
    "__`class Value`__ takes a single scalar value, wraps it, and represents it as __`Value(data=value)`__.\n",
    "\n",
    "For example, if do a value of 2.0 if will return a string. `Value(data=2.0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Value(2.0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is `grad=0` also appearing in result, it is because of multiple uses of `grad()` in previous code cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Linear Combination in `Value` Objects__\n",
    "Let's multiply two `Value` objects and add another `Value` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=Value(-1.0)\n",
    "c=Value(3.0)\n",
    "print(a*b+c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`a*b+c` in the above code cell will internally call:\n",
    ">``` (a.__mul__(b)).__add__(c) ```\n",
    "\n",
    "where `b,c` will be `other` respectively and `a` shall be `self`. Furthermore,\n",
    ">`Value(self.data+other.data)`,`Value(self.data*other.data)`\n",
    "\n",
    "is internally addition, multiplication of simple floating point objects respectively. This functions can be called as __operator overloading functions__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Computation Graph and Dependencies__:\n",
    "Here, we are missing the connections between nodes of this expression and are not tracking new values that are being produced with the previous values. This is one of the important things to be implemented for operations like __backpropagation__ to work where we traverse the graph in reverse to compute gradients.\n",
    "\n",
    "In frameworks like PyTorch or TensorFlow, such dependency tracking is used to implement autograd, enabling automatic differentiation by traversing the graph of operations.\n",
    "\n",
    "To implement this we introduce a new variable `children`, defaulting to an empty tuple, to track dependencies between values or it tracks which values produce others. Therefore the code shall be upgraded such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data=data\n",
    "        self._prev=set(_children)\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self,other):\n",
    "        return Value(self.data+other.data,(self,other))\n",
    "    def __mul__(self,other):\n",
    "        return Value(self.data*other.data,(self,other))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Changes in `Class Value`;\n",
    "Above at line number 2 we give an empty tuple as a parameter to the constructor, at line number 4 `self._prev=set(_children)` is given for storing the unique dependencies of the current Value object using a set, we also pass `(self,other)` to the __operator overloading functions__ (i.e., `__add__`, `__mul__`) to link them to `_children` tuple in line number 8,9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after passing the values of a, b, c..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=1.0)\n",
      "{Value(data=3.0), Value(data=-2.0)}\n"
     ]
    }
   ],
   "source": [
    "a=Value(2.0)\n",
    "b=Value(-1.0)\n",
    "c=Value(3.0)\n",
    "d=a*b+c\n",
    "print(f\"{d}\\n{d._prev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get `d` as `Value(data=1.0)` and `d._prev` as `{Value(data=3.0), Value(data=-2.0)}`, where `Value(data=-2.0)` is from `a*b` and `Value(data=3.0)` is from `c`. \n",
    "\n",
    "_Remember that,_\n",
    "- `d` returns the string representation of `d` because of the `__repr__` function\n",
    "- `d._prev` contains the dependencies of `d` which is a set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By here, we can track `Value` dependencies but not the operations that created them. So we need to make changes in the code so that we can also store the operations. These changes are necessary for precise __backpropagation__, and proper gradient attribution. Therefore, code shall be upgraded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self,data,_children=(),_op='',label=''):\n",
    "        self.data=data\n",
    "        self._prev=set(_children)\n",
    "        self._op=_op\n",
    "        self.label=label\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other):\n",
    "        return  Value(self.data+other.data,(self,other),'+')\n",
    "    def __mul__(self, other):\n",
    "         return Value(self.data*other.data,(self,other),'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are also tracking the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=1.0)\n",
      "{Value(data=3.0), Value(data=-2.0)}\n",
      "+\n"
     ]
    }
   ],
   "source": [
    "a=Value(2.0)\n",
    "b=Value(-1.0)\n",
    "c=Value(3.0)\n",
    "d=a*b+c\n",
    "print(f\"{d}\\n{d._prev}\\n{d._op}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `d._op` stores `'+'`, representing the last operation (`__add__`) performed to produce `d`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
